{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12800640,"sourceType":"datasetVersion","datasetId":8093421}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <center> <font size=\"+4\"> Fine-Tuning Hibiki - 2B </font> </center>\n\nThis notebook is an example of how to LoRA finetune Hibiki 2B. Recommended GPU is **A100 GPU**.\n\nCheck out `moshi-finetune` Github repo to learn more: https://github.com/kyutai-labs/moshi-finetune/\n\nTo replace the LLM with Gemma/LLaMa and still get streaming capabilities, check https://chatgpt.com/share/68a33f04-f260-8011-bc35-2b71120ffb9d as guideline\n","metadata":{"id":"RyuOCYM92LJb"}},{"cell_type":"markdown","source":"# <br> 1. Setup\n\nClone the `hibiki-finetune` repo from my profile `imrnh`:\n","metadata":{"id":"yxr8mv-17GfB"}},{"cell_type":"code","source":"# # Clone the repository\n# !git clone https://github.com/imrnh/hibiki-finetune.git\n\n# # Copy files to current directory (./)\n# !cp -r hibiki-finetune/* ./\n# !rm -rf hibiki-finetune\n\n# Install deps\n# %pip install -e ./","metadata":{"id":"TIj3IlIeVDIb","trusted":true,"execution":{"iopub.status.busy":"2025-08-22T13:29:05.780789Z","iopub.execute_input":"2025-08-22T13:29:05.781348Z","iopub.status.idle":"2025-08-22T13:29:05.785296Z","shell.execute_reply.started":"2025-08-22T13:29:05.781321Z","shell.execute_reply":"2025-08-22T13:29:05.784595Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"from contextlib import ExitStack\nimport fire\nimport torch.cuda\nimport torch.distributed as dist\nfrom torch.optim import AdamW, lr_scheduler\nimport os\nimport logging\nimport shutil\nimport pprint\nimport dataclasses\nimport copy\nimport yaml\nfrom pathlib import Path\nimport json\n\n# from torch.profiler import ProfilerActivity, profile\n\nfrom finetune.args import TrainArgs\nfrom finetune.checkpointing import Checkpointer\nfrom finetune.data.data_loader import build_data_loader\nfrom finetune.data.interleaver import InterleavedTokenizer, Interleaver\nfrom finetune.distributed import (\n    BACKEND,\n    avg_aggregate,\n    get_rank,\n    get_world_size,\n    is_torchrun,\n    set_device,\n)\nfrom finetune.eval import evaluate\nfrom finetune.loss import compute_loss_with_mask\nfrom finetune.mixed_precision import (\n    downcast_mixed_precision,\n    prepare_mixed_precision,\n    upcast_mixed_precision,\n)\nfrom finetune.monitoring.metrics_logger import (\n    MetricsLogger,\n    eval_log_msg,\n    get_eval_logs,\n    get_train_logs,\n    train_log_msg,\n)\nfrom finetune.monitoring.utils import set_logger\nfrom finetune.utils import TrainState, logged_closing, set_random_seed\nfrom finetune.wrapped_model import get_fsdp_model\nfrom moshi.models import loaders\nfrom moshi.conditioners import ConditionAttributes\nfrom moshi.modules.lora import LoRALinear\n\n\nlogger = logging.getLogger(\"train\")\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n\n# Set environment variables to simulate `torchrun` for a single process.\n# This is necessary for `dist.init_process_group` to work correctly.\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"MASTER_ADDR\"] = \"localhost\"\nos.environ[\"MASTER_PORT\"] = \"12355\" # A random free port\nos.environ[\"RANK\"] = \"0\"\nos.environ[\"WORLD_SIZE\"] = \"1\"\nos.environ[\"LOCAL_RANK\"] = \"0\"\n\nFRENCH_DATA_PATH = \"/kaggle/input/hibiki-stereo-annotated-en-fr/french_stereo.jsonl\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T13:29:05.808677Z","iopub.execute_input":"2025-08-22T13:29:05.808877Z","iopub.status.idle":"2025-08-22T13:29:13.180909Z","shell.execute_reply.started":"2025-08-22T13:29:05.808860Z","shell.execute_reply":"2025-08-22T13:29:13.180053Z"}},"outputs":[{"name":"stderr","text":"2025-08-22 13:29:09.580289: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755869349.602988    2809 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755869349.609913    2809 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## `yaml` File Generation","metadata":{}},{"cell_type":"code","source":"config = \"\"\"\n# data\ndata:\n  train_data: '/kaggle/input/hibiki-stereo-annotated-en-fr/english_stereo.jsonl'\n  eval_data: ''\n  shuffle: true\n\n# model\nmoshi_paths:\n  hf_repo_id: \"kyutai/hibiki-2b-pytorch-bf16\"\n\nfull_finetuning: false # Activate lora.enable if partial finetuning\nlora:\n  enable: true\n  rank: 128\n  scaling: 2.\n  ft_embed: false\n\n# training hyperparameters\nfirst_codebook_weight_multiplier: 100.\ntext_padding_weight: .5\n\n\n# tokens per training steps = batch_size x num_GPUs x duration_sec\n# we recommend a sequence duration of 300 seconds\n# If you run into memory error, you can try reduce the sequence length\nduration_sec: 12\nbatch_size: 2\nmax_steps: 15000\n\ngradient_checkpointing: true # Activate checkpointing of layers\n\n# optim\noptim:\n  lr: 5.e-7\n  weight_decay: 0.1\n  pct_start: 0.05\n\n# other\nseed: 0\neval_freq: 10000\ndo_eval: False\n\n\n# Checkpointing\nckpt_freq: 150\nlog_freq: 150\n\nsave_adapters: True\n\nrun_dir: \"save_dir\"\n\"\"\"","metadata":{"id":"5dxTlIQMaJGv","trusted":true,"execution":{"iopub.status.busy":"2025-08-22T13:29:13.182090Z","iopub.execute_input":"2025-08-22T13:29:13.182627Z","iopub.status.idle":"2025-08-22T13:29:13.186410Z","shell.execute_reply.started":"2025-08-22T13:29:13.182599Z","shell.execute_reply":"2025-08-22T13:29:13.185632Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# save the same file locally into the example.yaml file\nwith open(\"trainer.yaml\", \"w\") as file:\n    yaml.dump(yaml.safe_load(config), file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T13:29:13.187182Z","iopub.execute_input":"2025-08-22T13:29:13.187385Z","iopub.status.idle":"2025-08-22T13:29:13.214926Z","shell.execute_reply.started":"2025-08-22T13:29:13.187368Z","shell.execute_reply":"2025-08-22T13:29:13.214420Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# <br> 2. Fine-tune <br>","metadata":{}},{"cell_type":"code","source":"def main_logger_info(message: str) -> None:\n    if get_rank() == 0:\n        logger.info(message)\n\ndef get_condition_tensors(lm, batch_size: int, cfg_coef: float):\n    condition_tensors = {}\n    if lm.condition_provider is not None and lm.condition_provider.conditioners:\n        conditions: list[ConditionAttributes] | None = None\n        conditions = [\n            ConditionAttributes(text={\"description\": \"very_good\"}, tensor={})\n            for _ in range(batch_size)\n        ]\n        if cfg_coef != 1.0:\n            # Extending the conditions with the negatives for the CFG.\n            conditions += [\n                ConditionAttributes(text={\"description\": \"very_bad\"}, tensor={})\n                for _ in range(batch_size)\n            ]\n\n        assert conditions is not None\n        prepared = lm.condition_provider.prepare(conditions)\n        condition_tensors = lm.condition_provider(prepared)\n    return condition_tensors\n\n\ndef save_training_state(run_dir: Path, state: TrainState, optimizer, scheduler, step: int):\n    \"\"\"Save optimizer, scheduler, and training state for resuming.\"\"\"\n    training_state_dir = run_dir / \"checkpoints\" / f\"checkpoint_{step:06d}\" / \"consolidated\" / \"training_state\"\n    training_state_dir.mkdir(parents=True, exist_ok=True)\n    \n    if get_rank() == 0:  # Only save on rank 0\n        # Save optimizer state\n        torch.save(optimizer.state_dict(), training_state_dir / \"optimizer.pt\")\n        \n        # Save scheduler state\n        torch.save(scheduler.state_dict(), training_state_dir / \"scheduler.pt\")\n        \n        # Save training state (step, random states, etc.)\n        training_state = {\n            'step': state.step,\n            'torch_rng_state': torch.get_rng_state(),\n            'cuda_rng_state': torch.cuda.get_rng_state() if torch.cuda.is_available() else None,\n        }\n        torch.save(training_state, training_state_dir / \"training_state.pt\")\n        \n        main_logger_info(f\"Saved training state at step {step}\")\n\n\ndef load_training_state(checkpoint_path: str, state: TrainState, optimizer, scheduler, param_dtype):\n    \"\"\"Load optimizer, scheduler, and training state for resuming.\"\"\"\n    training_state_dir = Path(checkpoint_path) / \"training_state\"\n\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    \n    if not training_state_dir.exists():\n        main_logger_info(f\"No training state found at {training_state_dir}\")\n        return False\n    \n    try:\n        # Load training state\n        training_state_path = training_state_dir / \"training_state.pt\"\n        if training_state_path.exists():\n            training_state = torch.load(training_state_path, map_location='cpu')\n            state.step = training_state['step']\n            torch.set_rng_state(training_state['torch_rng_state'])\n            if training_state['cuda_rng_state'] is not None and torch.cuda.is_available():\n                torch.cuda.set_rng_state(training_state['cuda_rng_state'])\n            main_logger_info(f\"Resumed from step {state.step}\")\n        \n        # Load optimizer state\n        optimizer_path = training_state_dir / \"optimizer.pt\"\n        if optimizer_path.exists():\n            optimizer_state = torch.load(optimizer_path, map_location='cpu')\n            \n            # # Move optimizer state tensors to correct device and dtype\n            # for state in optimizer_state['state'].values():\n            #     for key, value in state.items():\n            #         if torch.is_tensor(value):\n            #             state[key] = value.cuda().to(param_dtype) if torch.cuda.is_available() else value.to(param_dtype)\n\n            # for state in optimizer_state['state'].values():\n            #     for k, v in state.items():\n            #         if torch.is_tensor(v):\n            #             v = v.to(torch.float32)   # keep optimizer state in fp32\n            #             if torch.cuda.is_available():\n            #                 v = v.cuda()\n            #             state[k] = v\n            # Force all optimizer state tensors to fp32 on correct device\n            for s in optimizer_state['state'].values():\n                for k, v in s.items():\n                    if torch.is_tensor(v):\n                        v = v.to(torch.float32)\n                        if torch.cuda.is_available():\n                            v = v.to(device)\n                        s[k] = v\n            \n            optimizer.load_state_dict(optimizer_state)\n            main_logger_info(\"Loaded optimizer state\")\n                \n        # Load scheduler state\n        scheduler_path = training_state_dir / \"scheduler.pt\"\n        if scheduler_path.exists():\n            device = 'cuda' if torch.cuda.is_available() else 'cpu'\n            scheduler_state = torch.load(scheduler_path, map_location=device)\n            scheduler.load_state_dict(scheduler_state)\n            main_logger_info(\"Loaded scheduler state\")\n        \n        return True\n    except Exception as e:\n        main_logger_info(f\"Failed to load training state: {e}\")\n        return False\n\n\ndef find_latest_checkpoint(run_dir: Path) -> str | None:\n    \"\"\"Find the latest checkpoint in the run directory.\"\"\"\n    checkpoints_dir = run_dir / \"checkpoints\"\n    if not checkpoints_dir.exists():\n        return None\n    \n    checkpoint_dirs = [d for d in checkpoints_dir.iterdir() if d.is_dir() and d.name.startswith(\"checkpoint_\")]\n    if not checkpoint_dirs:\n        return None\n    \n    # Sort by step number\n    checkpoint_dirs.sort(key=lambda x: int(x.name.split(\"_\")[1]))\n    latest_checkpoint = checkpoint_dirs[-1] / \"consolidated\" # Want to load training state from root.\n    \n    if latest_checkpoint.exists():\n        return str(latest_checkpoint)\n    return None\n\n\ndef load_lora_checkpoint(model, checkpoint_path: str):\n    \"\"\"Load LoRA weights from checkpoint.\"\"\"\n    lora_checkpoint_path = Path(checkpoint_path) / \"lora.safetensors\"\n    consolidated_checkpoint_path = Path(checkpoint_path) / \"consolidated.safetensors\"\n    \n    checkpoint_to_load = None\n    if lora_checkpoint_path.exists():\n        checkpoint_to_load = lora_checkpoint_path\n        main_logger_info(f\"Loading LoRA checkpoint from {checkpoint_to_load}\")\n    elif consolidated_checkpoint_path.exists():\n        checkpoint_to_load = consolidated_checkpoint_path\n        main_logger_info(f\"Loading consolidated checkpoint from {checkpoint_to_load}\")\n    else:\n        main_logger_info(f\"No checkpoint found at {checkpoint_path}\")\n        return False\n    \n    try:\n        import safetensors.torch\n        state_dict = safetensors.torch.load_file(checkpoint_to_load)\n        \n        # Load the state dict (this will load only the LoRA parameters if it's a LoRA checkpoint)\n        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n        \n        if missing_keys:\n            main_logger_info(f\"Missing keys when loading checkpoint: {missing_keys[:10]}...\")  # Show first 10\n        if unexpected_keys:\n            main_logger_info(f\"Unexpected keys when loading checkpoint: {unexpected_keys[:10]}...\")  \n            \n        main_logger_info(\"Successfully loaded model checkpoint | Log from function: load_lora_checkpoint\")\n        return True\n    except Exception as e:\n        main_logger_info(f\"Failed to load checkpoint: {e}\")\n        return False\n\n\ndef train(config: str):\n    args: TrainArgs = TrainArgs.load(config, drop_extra_fields=False)\n    \n    # Add resume_from_checkpoint attribute to args if it doesn't exist\n    if not hasattr(args, 'resume_from_checkpoint'):\n        args.resume_from_checkpoint = None\n    \n    set_logger(logging.INFO)\n\n    with ExitStack() as exit_stack:\n        _train(args, exit_stack)\n    logger.info(\"Closed everything!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T13:29:13.216524Z","iopub.execute_input":"2025-08-22T13:29:13.216734Z","iopub.status.idle":"2025-08-22T13:29:13.236586Z","shell.execute_reply.started":"2025-08-22T13:29:13.216718Z","shell.execute_reply":"2025-08-22T13:29:13.236052Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"the_model = None\nthe_optimizer = None\n\ndef _train(args: TrainArgs, exit_stack: ExitStack):\n    # 1. Initial setup and checks\n    global the_model\n    global the_optimizer\n    set_random_seed(args.seed)\n    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n   \n    # Init NCCL\n    if \"LOCAL_RANK\" in os.environ:\n        set_device()\n        logger.info(\"Going to init comms...\")\n        dist.init_process_group(backend=BACKEND)\n    else:\n        logger.error(\"PyTorch environment is not correctly initialized. This message should only be displayed when testing.\")\n\n    \n    # 2. Init run dir\n    main_logger_info(f\"Run dir: {args.run_dir}\")\n    run_dir = Path(args.run_dir)\n\n    # Check for resume_from_checkpoint in args or find latest checkpoint\n    resume_from_checkpoint = find_latest_checkpoint(run_dir) # set to path of checkpoint.\n\n    if hasattr(args, 'resume_from_checkpoint') and args.resume_from_checkpoint:\n        resume_checkpoint = args.resume_from_checkpoint\n        main_logger_info(f\"\\n****Resuming from specified checkpoint: {resume_checkpoint}****\\n\")\n    else:\n        # Try to find latest checkpoint\n        resume_checkpoint = find_latest_checkpoint(run_dir)\n        if resume_checkpoint:\n            main_logger_info(f\"Found latest checkpoint to resume from: {resume_checkpoint}\")\n\n    if is_torchrun() and not resume_checkpoint:\n        if run_dir.exists():\n            main_logger_info(f\"Removing run dir {run_dir}...\")\n            shutil.rmtree(run_dir)\n\n    if args.full_finetuning:\n        assert not args.lora.enable, \"LoRA should not be enabled for full finetuning.\"\n    else:\n        assert args.lora.enable, \"LoRA should be enabled for partial finetuning\"\n\n    dist.barrier()\n    run_dir.mkdir(exist_ok=True, parents=True)\n\n    args_path = run_dir / \"args.yaml\"\n    if not args_path.exists():\n        args.save(args_path)\n\n    main_logger_info(f\"TrainArgs: {pprint.pformat(dataclasses.asdict(args))}\")\n\n    # 3. Get loggers\n    metrics_logger: MetricsLogger = MetricsLogger(\n        run_dir,\n        tag=\"train\",\n        is_master=get_rank() == 0,\n        wandb_args=args.wandb,\n        config=dataclasses.asdict(args),\n    )\n    exit_stack.enter_context(logged_closing(metrics_logger, \"metrics_logger\"))\n\n    eval_logger: MetricsLogger = MetricsLogger(\n        run_dir,\n        tag=\"eval\",\n        is_master=get_rank() == 0,\n        wandb_args=args.wandb,\n        config=dataclasses.asdict(args),\n    )\n    exit_stack.enter_context(logged_closing(eval_logger, \"eval_logger\"))\n\n    # 4.1 Load function calling audio encoder and tokenizer\n    main_logger_info(\"Loading Mimi and Moshi...\")\n    checkpoint_info = loaders.CheckpointInfo.from_hf_repo(\n        hf_repo=args.moshi_paths.hf_repo_id,\n        moshi_weights=args.moshi_paths.moshi_path,\n        mimi_weights=args.moshi_paths.mimi_path,\n        tokenizer=args.moshi_paths.tokenizer_path,\n        config_path=args.moshi_paths.config_path,\n    )\n\n    lm_config = (\n        loaders._lm_kwargs\n        if checkpoint_info.raw_config is None\n        else checkpoint_info.raw_config\n    )\n    lm_config[\"lora\"] = args.lora.enable\n    lm_config[\"lora_rank\"] = args.lora.rank\n    lm_config[\"lora_scaling\"] = args.lora.scaling\n\n    mimi = checkpoint_info.get_mimi(device=\"cuda\")\n    mimi.eval()\n    for p in mimi.parameters():\n        p.requires_grad = False\n\n    # 4.2 Load and shard model, prepare interleaver for audio/text tokens.\n    model = get_fsdp_model(args, checkpoint_info)\n\n    # Load checkpoint if resuming\n    if resume_checkpoint:\n        success = load_lora_checkpoint(model, resume_checkpoint)\n        if not success:\n            main_logger_info(\"Failed to load checkpoint, starting from scratch\")\n            resume_checkpoint = None\n\n    spm = checkpoint_info.get_text_tokenizer()\n\n    interleaver = Interleaver(\n        spm,\n        mimi.frame_rate,\n        model.text_padding_token_id,\n        model.end_of_text_padding_id,\n        model.zero_token_id,\n        keep_main_only=True,\n    )\n    interleaved_tokenizer = InterleavedTokenizer(\n        mimi, interleaver, duration_sec=args.duration_sec\n    )\n\n    # 5. Load data loaders\n    data_loader = build_data_loader(\n        instruct_tokenizer=interleaved_tokenizer,\n        args=args.data,\n        batch_size=args.batch_size,\n        seed=args.seed,\n        rank=get_rank(),  # DDP rank\n        world_size=get_world_size(),  # DDP world_size\n        is_eval=False,\n    )\n\n    french_data_args = copy.deepcopy(args.data)\n    french_data_args.train_data = FRENCH_DATA_PATH\n    \n    print(f\"French data args: {french_data_args}\")\n\n    french_data_loader = build_data_loader(\n        instruct_tokenizer=interleaved_tokenizer,\n        args=french_data_args,\n        batch_size=args.batch_size,\n        seed=args.seed,\n        rank=get_rank(),  # DDP rank\n        world_size=get_world_size(),  # DDP world_size\n        is_eval=False,\n    )\n\n    saved_tokenizer = interleaved_tokenizer\n\n    if args.do_eval:\n        eval_data_loader = build_data_loader(\n            instruct_tokenizer=interleaved_tokenizer,\n            args=args.data,\n            batch_size=args.batch_size,\n            seed=None,\n            rank=get_rank(),  # DDP rank\n            world_size=get_world_size(),  # DDP world_size\n            is_eval=True,\n        )\n\n    # 6. Load model\n    # Define mixed precision\n    param_dtype = getattr(torch, args.param_dtype)\n    optim_dtype = torch.bfloat16\n\n    # cast_lora_params(model, param_dtype)\n    for m in model.modules():\n        if isinstance(m, LoRALinear):\n            m.lora_A.to(dtype=torch.bfloat16)\n            m.lora_B.to(dtype=torch.bfloat16)\n\n    assert args.lora is not None, \"`args.lora` should be set to a valid value.\"\n    \n    # # Ensure LoRA trainable params are cast to the same dtype as the model\n    # for name, p in model.named_parameters():\n    #     if p.requires_grad:\n    #         p.data = p.data.to(param_dtype)\n    #         if p.grad is not None:\n    #             p.grad = p.grad.to(param_dtype)\n    # print(f\"Changed Model parameters to {param_dtype}\")\n    # model = model.to(dtype=param_dtype)\n    # print(f\"Model to {param_dtype}\")\n\n    # 7. Load optimizer\n    optimizer = AdamW(\n        model.parameters(),\n        lr=args.optim.lr,\n        betas=(0.9, 0.95),\n        eps=1e-08,\n        weight_decay=args.optim.weight_decay,\n    )\n\n    scheduler = lr_scheduler.OneCycleLR(\n        optimizer,\n        max_lr=args.optim.lr,\n        total_steps=args.max_steps,\n        pct_start=args.optim.pct_start,\n    )\n\n    state = TrainState(args.max_steps)\n\n    # Load training state if resuming\n    if resume_checkpoint:\n        success = load_training_state(resume_checkpoint, state, optimizer, scheduler, param_dtype)\n        if success:\n            main_logger_info(f\"Successfully resumed training from step {state.step}\")\n        else:\n            main_logger_info(\"Failed to load training state, starting from scratch\")\n\n    # 8. Initialize checkpointer\n    if args.do_ckpt:\n        checkpointer = Checkpointer(\n            model=model,\n            state=state,\n            config=lm_config,\n            run_dir=run_dir,\n            optimizer=optimizer,\n            num_ckpt_keep=args.num_ckpt_keep,\n            full_finetuning=args.full_finetuning,\n        )\n        \n    # 9. Prepare mixed precision\n    prepare_mixed_precision(\n        model.parameters(), param_dtype=param_dtype, optim_dtype=optim_dtype\n    )\n\n    for gi, group in enumerate(optimizer.param_groups):\n        for pi, p in enumerate(group['params']):\n            group['params'][pi] = p.to(optim_dtype)  # re-assign in param group\n\n    # 11. train!\n    model.train()\n    torch.cuda.empty_cache()\n\n    while state.step < args.max_steps:\n        state.start_step()\n        is_last_step = state.step == args.max_steps\n\n        optimizer.zero_grad()\n\n        loss = torch.tensor([0.0], device=\"cuda\")\n        n_batch_tokens: int = 0\n        n_real_tokens: int = 0\n\n\n        for i in range(args.num_microbatches):\n            batch = next(data_loader)\n            codes = batch.codes\n            \n            fr_batch = next(french_data_loader)\n            fr_codes = fr_batch.codes\n\n            condition_tensors = None\n            if batch.condition_attributes is not None:\n                condition_tensors = model.condition_provider.prepare(\n                    batch.condition_attributes\n                )\n\n            if condition_tensors is None:\n                condition_tensors = get_condition_tensors(model, args.batch_size, 1.0)\n\n                    \n            the_model = model\n            the_optimizer = optimizer\n        \n\n            # forward / backward\n            output = model(codes=codes, condition_tensors=condition_tensors)\n\n            text_loss_component = fr_codes[:, : model.audio_offset]\n            audio_loss_component = fr_codes[:, model.audio_offset : model.audio_offset + model.dep_q]\n\n            # Replace all -1 with -100\n            text_loss_component = torch.where(\n                text_loss_component == -1, \n                torch.tensor(-100, dtype=text_loss_component.dtype, device=text_loss_component.device), \n                text_loss_component\n            )\n\n            # Replace all -1 with -100\n            audio_loss_component = torch.where(\n                audio_loss_component == -1, \n                torch.tensor(-100, dtype=audio_loss_component.dtype, device=audio_loss_component.device), \n                audio_loss_component\n            )\n            \n            \n            text_loss = compute_loss_with_mask(\n                output.text_logits,\n                text_loss_component,\n                output.text_mask,\n                mode=\"text\",\n                text_padding_weight=args.text_padding_weight,\n                text_padding_ids={\n                    model.text_padding_token_id,\n                    model.end_of_text_padding_id,\n                },\n            )\n            \n            audio_loss = compute_loss_with_mask(\n                output.logits,\n                audio_loss_component,\n                output.mask,\n                mode=\"audio\",\n                first_codebook_weight_multiplier=args.first_codebook_weight_multiplier,\n            )\n\n            mb_loss = text_loss + audio_loss\n            mb_loss.backward()\n\n            loss += mb_loss.detach()\n            n_batch_tokens += output.text_mask.numel() + output.mask.numel()\n            n_real_tokens += (\n                torch.sum(output.text_mask).item() + torch.sum(output.mask).item()\n            )\n\n            if i < args.num_microbatches - 1:\n                # synchronize CUDA to re-run backward\n                assert args.num_microbatches > 1  # should not happen\n                torch.cuda.synchronize()\n\n        if args.num_microbatches > 1:\n            loss /= args.num_microbatches\n            for p in model.parameters():\n                if p.requires_grad:\n                    assert p.grad is not None\n                    p.grad.div_(args.num_microbatches)\n\n        # upcast params for optimizer update\n        upcast_mixed_precision(model.parameters(), optim_dtype=optim_dtype)\n\n        # clip grad norm\n        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_norm)\n\n        # optimizer step\n        optimizer.step()\n\n        # downcast params for forward & backward\n        downcast_mixed_precision(model.parameters(), param_dtype=param_dtype)\n\n        last_lr = scheduler.get_last_lr()[0]\n        scheduler.step()\n\n        # Host sync\n        loss_item = loss.item()\n        avg_loss = avg_aggregate(loss_item)\n\n        if args.do_eval and ((args.eval_freq > 0 and state.step % args.eval_freq == 0) or is_last_step):\n            # write perplexity to state\n            evaluate(model, eval_data_loader, state, args)\n\n            eval_logs = get_eval_logs(state.step, avg_loss, state.this_eval_perplexity, state.this_eval_loss,)\n\n            main_logger_info(eval_log_msg(eval_logs))\n            eval_logger.log(eval_logs, step=state.step)\n\n        # Timing\n        state.end_step(n_batch_tokens)\n\n        if state.step % args.log_freq == 0:\n            train_logs = get_train_logs(state, avg_loss, n_real_tokens, last_lr, torch.cuda.max_memory_allocated(), torch.cuda.memory_allocated(), args,)\n            main_logger_info(train_log_msg(state, logs=train_logs, loss=avg_loss))\n            metrics_logger.log(train_logs, step=state.step)\n\n        if args.do_ckpt and ((args.ckpt_freq > 0 and state.step % args.ckpt_freq == 0) or is_last_step):\n            checkpointer.save_checkpoint(save_only_lora=not args.full_finetuning and args.save_adapters, dtype=param_dtype,)\n            \n            # Save training state (optimizer, scheduler, random states)\n            save_training_state(run_dir, state, optimizer, scheduler, state.step)\n\n    main_logger_info(\"done!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T13:29:13.237234Z","iopub.execute_input":"2025-08-22T13:29:13.237424Z","iopub.status.idle":"2025-08-22T13:29:13.262604Z","shell.execute_reply.started":"2025-08-22T13:29:13.237409Z","shell.execute_reply":"2025-08-22T13:29:13.261907Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Start Training","metadata":{}},{"cell_type":"code","source":"train(config='trainer.yaml')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clean up the distributed environment\nif dist.is_initialized():\n    dist.destroy_process_group()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T13:31:58.025695Z","iopub.status.idle":"2025-08-22T13:31:58.026006Z","shell.execute_reply.started":"2025-08-22T13:31:58.025859Z","shell.execute_reply":"2025-08-22T13:31:58.025877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Check first 2 model params\n# for i, (name, p) in enumerate(the_model.named_parameters()):\n#     if p.requires_grad:\n#         print(f\"Model param[{i}]: {name}, device={p.device}, dtype={p.dtype}\")\n#     if i >= 1:  # stop after 2\n#         break\n\n# # # Check first 2 optimizer params\n# for gi, group in enumerate(the_optimizer.param_groups):\n#     # for pi, p in enumerate(group['params']):\n#     #     if p.grad is not None or p.requires_grad:\n#     #         print(f\"Optimizer param[{gi}:{pi}]: device={p.device}, dtype={p.dtype}\")\n#     #     if pi >= 1:  # stop after 2 per group\n#     #         break\n#     # if gi >= 0:  # only first group\n#     #     break\n#     for p in group['params']:\n#         print(p.dtype, p.requires_grad)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T13:31:58.029602Z","iopub.status.idle":"2025-08-22T13:31:58.029917Z","shell.execute_reply.started":"2025-08-22T13:31:58.029758Z","shell.execute_reply":"2025-08-22T13:31:58.029774Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Update optimizer param type.\n\n# for gi, group in enumerate(the_optimizer.param_groups):\n#     for pi, p in enumerate(group['params']):\n#         if p.requires_grad:\n#             group['params'][pi] = p.to(torch.bfloat16)  # re-assign in param group","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T13:31:58.030579Z","iopub.status.idle":"2025-08-22T13:31:58.030796Z","shell.execute_reply.started":"2025-08-22T13:31:58.030683Z","shell.execute_reply":"2025-08-22T13:31:58.030691Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for gi, group in enumerate(the_optimizer.param_groups):\n#     # for pi, p in enumerate(group['params']):\n#     #     if p.grad is not None or p.requires_grad:\n#     #         print(f\"Optimizer param[{gi}:{pi}]: device={p.device}, dtype={p.dtype}\")\n#     #     if pi >= 1:  # stop after 2 per group\n#     #         break\n#     # if gi >= 0:  # only first group\n#     #     break\n#     for p in group['params']:\n#         print(p.dtype, p.requires_grad)\n#         # p.to(torch.bfloat16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T13:31:58.031717Z","iopub.status.idle":"2025-08-22T13:31:58.032013Z","shell.execute_reply.started":"2025-08-22T13:31:58.031836Z","shell.execute_reply":"2025-08-22T13:31:58.031851Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Debug Mode","metadata":{}},{"cell_type":"code","source":"# \"\"\"\n# dict_keys(['en_codes', 'fr_codes', 'text_logits', 'text_loss_component', 'text_mask', 'text_padding_weight', \n#     'text_padding_token_id', 'end_of_text_padding_id', 'logits', 'audio_loss_component', 'output_mask', \n#     'first_codebook_weight_multiplier'])\n# \"\"\"\n\n\n# gcs = gen_codes[-2]\n\n# with torch.enable_grad():\n#     text_loss = compute_loss_with_mask(\n#         gcs['text_logits'],\n#         gcs['text_loss_component'],\n#         gcs['text_mask'],\n#         mode=\"text\",\n#         text_padding_weight=gcs['text_padding_weight'],\n#         text_padding_ids={\n#             gcs['text_padding_token_id'],\n#             gcs['end_of_text_padding_id'],\n#         },\n#     )\n#     audio_loss = compute_loss_with_mask(\n#         gcs['logits'],\n#         gcs['audio_loss_component'],\n#         gcs['output_mask'],\n#         mode=\"audio\",\n#         first_codebook_weight_multiplier=gcs['first_codebook_weight_multiplier'],\n#     )\n    \n#     print(text_loss)\n#     print(audio_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-22T13:31:58.033386Z","iopub.status.idle":"2025-08-22T13:31:58.033607Z","shell.execute_reply.started":"2025-08-22T13:31:58.033501Z","shell.execute_reply":"2025-08-22T13:31:58.033510Z"}},"outputs":[],"execution_count":null}]}