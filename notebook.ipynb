{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12790859,"sourceType":"datasetVersion","datasetId":8086994}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Getting Started with Fine-Tuning Hibiki - 2B\n\nThis notebook is an example of how to LoRA finetune Hibiki 2B. Recommended GPU is **A100 GPU**.\n\nCheck out `moshi-finetune` Github repo to learn more: https://github.com/kyutai-labs/moshi-finetune/\n\nTo replace the LLM with Gemma/LLaMa and still get streaming capabilities, check https://chatgpt.com/share/68a33f04-f260-8011-bc35-2b71120ffb9d as guideline\n","metadata":{"id":"RyuOCYM92LJb"}},{"cell_type":"markdown","source":"# 1. Setup\n\nClone the `hibiki-finetune` repo from my profile `imrnh`:\n","metadata":{"id":"yxr8mv-17GfB"}},{"cell_type":"code","source":"# # Clone the repository\n# !git clone https://github.com/imrnh/hibiki-finetune.git\n\n# # Copy files to current directory (./)\n# !cp -r hibiki-finetune/* ./\n# !rm -rf hibiki-finetune\n\n# Install deps\n%pip install -e ./","metadata":{"id":"TIj3IlIeVDIb","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import dataclasses\nimport logging\nimport os\nimport copy\nimport pprint\nimport shutil\nfrom contextlib import ExitStack\nfrom pathlib import Path\n\nimport fire\nimport torch.cuda\nimport torch.distributed as dist\nfrom torch.optim import AdamW, lr_scheduler\n\n# from torch.profiler import ProfilerActivity, profile\n\nfrom finetune.args import TrainArgs\nfrom finetune.checkpointing import Checkpointer\nfrom finetune.data.data_loader import build_data_loader\nfrom finetune.data.interleaver import InterleavedTokenizer, Interleaver\nfrom finetune.distributed import (\n    BACKEND,\n    avg_aggregate,\n    get_rank,\n    get_world_size,\n    is_torchrun,\n    set_device,\n)\nfrom finetune.eval import evaluate\nfrom finetune.loss import compute_loss_with_mask\nfrom finetune.mixed_precision import (\n    downcast_mixed_precision,\n    prepare_mixed_precision,\n    upcast_mixed_precision,\n)\nfrom finetune.monitoring.metrics_logger import (\n    MetricsLogger,\n    eval_log_msg,\n    get_eval_logs,\n    get_train_logs,\n    train_log_msg,\n)\nfrom finetune.monitoring.utils import set_logger\nfrom finetune.utils import TrainState, logged_closing, set_random_seed\nfrom finetune.wrapped_model import get_fsdp_model\nfrom moshi.models import loaders\nfrom moshi.conditioners import ConditionAttributes\n\n\nlogger = logging.getLogger(\"train\")\n\n\n# Set environment variables to simulate `torchrun` for a single process.\n# This is necessary for `dist.init_process_group` to work correctly.\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"MASTER_ADDR\"] = \"localhost\"\nos.environ[\"MASTER_PORT\"] = \"12355\" # A random free port\nos.environ[\"RANK\"] = \"0\"\nos.environ[\"WORLD_SIZE\"] = \"1\"\nos.environ[\"LOCAL_RANK\"] = \"0\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"FRENCH_DATA_PATH = \"hibiki_data/stereo/french_stereo.jsonl\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## `yaml` File Generation","metadata":{}},{"cell_type":"code","source":"config = \"\"\"\n# data\ndata:\n  train_data: 'hibiki_data/stereo/english_stereo.jsonl'\n  eval_data: ''\n  shuffle: true\n\n# model\nmoshi_paths:\n  hf_repo_id: \"kyutai/hibiki-2b-pytorch-bf16\"\n\n\nfull_finetuning: false # Activate lora.enable if partial finetuning\nlora:\n  enable: true\n  rank: 128\n  scaling: 2.\n  ft_embed: false\n\n# training hyperparameters\nfirst_codebook_weight_multiplier: 100.\ntext_padding_weight: .5\n\n\n# tokens per training steps = batch_size x num_GPUs x duration_sec\n# we recommend a sequence duration of 300 seconds\n# If you run into memory error, you can try reduce the sequence length\nduration_sec: 100\nbatch_size: 1\nmax_steps: 300\n\ngradient_checkpointing: true # Activate checkpointing of layers\n\n# optim\noptim:\n  lr: 2.e-6\n  weight_decay: 0.1\n  pct_start: 0.05\n\n# other\nseed: 0\nlog_freq: 10\neval_freq: 1\ndo_eval: False\nckpt_freq: 10\n\nsave_adapters: True\n\nrun_dir: \"run\"\n\"\"\"","metadata":{"id":"5dxTlIQMaJGv","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# save the same file locally into the example.yaml file\nimport yaml\nwith open(\"trainer.yaml\", \"w\") as file:\n    yaml.dump(yaml.safe_load(config), file)\n\n! rm -r run","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fine-tune","metadata":{}},{"cell_type":"code","source":"def main_logger_info(message: str) -> None:\n    if get_rank() == 0:\n        logger.info(message)\n\ndef get_condition_tensors(lm, batch_size: int, cfg_coef: float):\n    condition_tensors = {}\n    if lm.condition_provider is not None and lm.condition_provider.conditioners:\n        conditions: list[ConditionAttributes] | None = None\n        conditions = [\n            ConditionAttributes(text={\"description\": \"very_good\"}, tensor={})\n            for _ in range(batch_size)\n        ]\n        if cfg_coef != 1.0:\n            # Extending the conditions with the negatives for the CFG.\n            conditions += [\n                ConditionAttributes(text={\"description\": \"very_bad\"}, tensor={})\n                for _ in range(batch_size)\n            ]\n\n        assert conditions is not None\n        prepared = lm.condition_provider.prepare(conditions)\n        condition_tensors = lm.condition_provider(prepared)\n    return condition_tensors\n\n\ndef train(config: str):\n    args: TrainArgs = TrainArgs.load(config, drop_extra_fields=False)\n    set_logger(logging.INFO)\n\n    with ExitStack() as exit_stack:\n        _train(args, exit_stack)\n    logger.info(\"Closed everything!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def _train(args: TrainArgs, exit_stack: ExitStack):\n    # 1. Initial setup and checks\n    set_random_seed(args.seed)\n    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n    # Init NCCL\n    if \"LOCAL_RANK\" in os.environ:\n        set_device()\n        logger.info(\"Going to init comms...\")\n        dist.init_process_group(backend=BACKEND)\n    else:\n        logger.error(\"PyTorch environment is not correctly initialized. This message should only be displayed when testing.\")\n\n    \n    # 2. Init run dir\n    main_logger_info(f\"Run dir: {args.run_dir}\")\n    run_dir = Path(args.run_dir)\n\n    if is_torchrun():\n        if run_dir.exists():\n            main_logger_info(f\"Removing run dir {run_dir}...\")\n            shutil.rmtree(run_dir)\n\n    if args.full_finetuning:\n        assert not args.lora.enable, \"LoRA should not be enabled for full finetuning.\"\n    else:\n        assert args.lora.enable, \"LoRA should be enabled for partial finetuning\"\n\n    dist.barrier()\n    run_dir.mkdir(exist_ok=True, parents=True)\n\n    args_path = run_dir / \"args.yaml\"\n    if not args_path.exists():\n        args.save(args_path)\n\n    main_logger_info(f\"TrainArgs: {pprint.pformat(dataclasses.asdict(args))}\")\n\n    # 3. Get loggers\n    metrics_logger: MetricsLogger = MetricsLogger(\n        run_dir,\n        tag=\"train\",\n        is_master=get_rank() == 0,\n        wandb_args=args.wandb,\n        config=dataclasses.asdict(args),\n    )\n    exit_stack.enter_context(logged_closing(metrics_logger, \"metrics_logger\"))\n\n    eval_logger: MetricsLogger = MetricsLogger(\n        run_dir,\n        tag=\"eval\",\n        is_master=get_rank() == 0,\n        wandb_args=args.wandb,\n        config=dataclasses.asdict(args),\n    )\n    exit_stack.enter_context(logged_closing(eval_logger, \"eval_logger\"))\n\n    # 4.1 Load function calling audio encoder and tokenizer\n    main_logger_info(\"Loading Mimi and Moshi...\")\n    checkpoint_info = loaders.CheckpointInfo.from_hf_repo(\n        hf_repo=args.moshi_paths.hf_repo_id,\n        moshi_weights=args.moshi_paths.moshi_path,\n        mimi_weights=args.moshi_paths.mimi_path,\n        tokenizer=args.moshi_paths.tokenizer_path,\n        config_path=args.moshi_paths.config_path,\n    )\n\n    lm_config = (\n        loaders._lm_kwargs\n        if checkpoint_info.raw_config is None\n        else checkpoint_info.raw_config\n    )\n    lm_config[\"lora\"] = args.lora.enable\n    lm_config[\"lora_rank\"] = args.lora.rank\n    lm_config[\"lora_scaling\"] = args.lora.scaling\n\n    mimi = checkpoint_info.get_mimi(device=\"cuda\")\n    mimi.eval()\n    for p in mimi.parameters():\n        p.requires_grad = False\n\n    # 4.2 Load and shard model, prepare interleaver for audio/text tokens.\n    model = get_fsdp_model(args, checkpoint_info)\n\n    spm = checkpoint_info.get_text_tokenizer()\n\n    interleaver = Interleaver(\n        spm,\n        mimi.frame_rate,\n        model.text_padding_token_id,\n        model.end_of_text_padding_id,\n        model.zero_token_id,\n        keep_main_only=True,\n    )\n    interleaved_tokenizer = InterleavedTokenizer(\n        mimi, interleaver, duration_sec=args.duration_sec\n    )\n\n    # 5. Load data loaders\n    data_loader = build_data_loader(\n        instruct_tokenizer=interleaved_tokenizer,\n        args=args.data,\n        batch_size=args.batch_size,\n        seed=args.seed,\n        rank=get_rank(),  # DDP rank\n        world_size=get_world_size(),  # DDP world_size\n        is_eval=False,\n    )\n\n    french_data_args = copy.deepcopy(args.data)\n    french_data_args.train_data = FRENCH_DATA_PATH\n    \n    print(f\"French data args: {french_data_args}\")\n\n    french_data_loader = build_data_loader(\n        instruct_tokenizer=interleaved_tokenizer,\n        args=french_data_args,\n        batch_size=args.batch_size,\n        seed=args.seed,\n        rank=get_rank(),  # DDP rank\n        world_size=get_world_size(),  # DDP world_size\n        is_eval=False,\n    )\n\n    saved_tokenizer = interleaved_tokenizer\n\n    if args.do_eval:\n        eval_data_loader = build_data_loader(\n            instruct_tokenizer=interleaved_tokenizer,\n            args=args.data,\n            batch_size=args.batch_size,\n            seed=None,\n            rank=get_rank(),  # DDP rank\n            world_size=get_world_size(),  # DDP world_size\n            is_eval=True,\n        )\n\n    # 6. Load model\n    # Define mixed precision\n    param_dtype = getattr(torch, args.param_dtype)\n    optim_dtype = torch.float32\n\n    assert args.lora is not None, \"`args.lora` should be set to a valid value.\"\n\n    # 7. Load optimizer\n    optimizer = AdamW(\n        model.parameters(),\n        lr=args.optim.lr,\n        betas=(0.9, 0.95),\n        eps=1e-08,\n        weight_decay=args.optim.weight_decay,\n    )\n\n    scheduler = lr_scheduler.OneCycleLR(\n        optimizer,\n        max_lr=args.optim.lr,\n        total_steps=args.max_steps,\n        pct_start=args.optim.pct_start,\n    )\n\n    state = TrainState(args.max_steps)\n\n    # 8. Initialize checkpointer\n    if args.do_ckpt:\n        checkpointer = Checkpointer(\n            model=model,\n            state=state,\n            config=lm_config,\n            run_dir=run_dir,\n            optimizer=optimizer,\n            num_ckpt_keep=args.num_ckpt_keep,\n            full_finetuning=args.full_finetuning,\n        )\n        \n    # 9. Prepare mixed precision\n    prepare_mixed_precision(\n        model.parameters(), param_dtype=param_dtype, optim_dtype=optim_dtype\n    )\n\n    # 11. train!\n    model.train()\n    torch.cuda.empty_cache()\n\n    \n    while state.step < args.max_steps:\n        state.start_step()\n        is_last_step = state.step == args.max_steps\n\n        optimizer.zero_grad()\n\n        loss = torch.tensor([0.0], device=\"cuda\")\n        n_batch_tokens: int = 0\n        n_real_tokens: int = 0\n\n        for i in range(args.num_microbatches):\n            batch = next(data_loader)\n            codes = batch.codes\n            \n            fr_batch = next(french_data_loader)\n            fr_codes = fr_batch.codes\n\n\n            condition_tensors = None\n            if batch.condition_attributes is not None:\n                condition_tensors = model.condition_provider.prepare(\n                    batch.condition_attributes\n                )\n\n            if condition_tensors is None:\n                condition_tensors = get_condition_tensors(model, args.batch_size, 1.0)\n\n            # forward / backward\n            output = model(codes=codes, condition_tensors=condition_tensors)\n            text_loss = compute_loss_with_mask(\n                output.text_logits,\n                fr_codes[:, : model.audio_offset],\n                output.text_mask,\n                mode=\"text\",\n                text_padding_weight=args.text_padding_weight,\n                text_padding_ids={\n                    model.text_padding_token_id,\n                    model.end_of_text_padding_id,\n                },\n            )\n            audio_loss = compute_loss_with_mask(\n                output.logits,\n                fr_codes[:, model.audio_offset : model.audio_offset + model.dep_q],\n                output.mask,\n                mode=\"audio\",\n                first_codebook_weight_multiplier=args.first_codebook_weight_multiplier,\n            )\n\n            mb_loss = text_loss + audio_loss\n            mb_loss.backward()\n\n            loss += mb_loss.detach()\n            n_batch_tokens += output.text_mask.numel() + output.mask.numel()\n            n_real_tokens += (\n                torch.sum(output.text_mask).item() + torch.sum(output.mask).item()\n            )\n\n            if i < args.num_microbatches - 1:\n                # synchronize CUDA to re-run backward\n                assert args.num_microbatches > 1  # should not happen\n                torch.cuda.synchronize()\n\n        if args.num_microbatches > 1:\n            loss /= args.num_microbatches\n            for p in model.parameters():\n                if p.requires_grad:\n                    assert p.grad is not None\n                    p.grad.div_(args.num_microbatches)\n\n        # upcast params for optimizer update\n        upcast_mixed_precision(model.parameters(), optim_dtype=optim_dtype)\n\n        # clip grad norm\n        torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_norm)\n\n        # optimizer step\n        optimizer.step()\n\n        # downcast params for forward & backward\n        downcast_mixed_precision(model.parameters(), param_dtype=param_dtype)\n\n        last_lr = scheduler.get_last_lr()[0]\n        scheduler.step()\n\n        # Host sync\n        loss_item = loss.item()\n        avg_loss = avg_aggregate(loss_item)\n\n        if args.do_eval and ((args.eval_freq > 0 and state.step % args.eval_freq == 0) or is_last_step):\n            # write perplexity to state\n            evaluate(model, eval_data_loader, state, args)\n\n            eval_logs = get_eval_logs(state.step, avg_loss, state.this_eval_perplexity, state.this_eval_loss,)\n\n            main_logger_info(eval_log_msg(eval_logs))\n            eval_logger.log(eval_logs, step=state.step)\n\n        # Timing\n        state.end_step(n_batch_tokens)\n\n        if state.step % args.log_freq == 0:\n            train_logs = get_train_logs(state, avg_loss, n_real_tokens, last_lr, torch.cuda.max_memory_allocated(), torch.cuda.memory_allocated(), args,)\n            main_logger_info(train_log_msg(state, logs=train_logs, loss=avg_loss))\n            metrics_logger.log(train_logs, step=state.step)\n\n        if args.do_ckpt and ((args.ckpt_freq > 0 and state.step % args.ckpt_freq == 0) or is_last_step):\n            checkpointer.save_checkpoint(save_only_lora=not args.full_finetuning and args.save_adapters, dtype=param_dtype,)\n\n    main_logger_info(\"done!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train(config='trainer.yaml')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clean up the distributed environment\nif dist.is_initialized():\n    dist.destroy_process_group()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}